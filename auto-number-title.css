# 超融合架构

#### 思考的基础

##### 硬件能力是软件架构的基础，再先进的软件架构，也不能超越硬件能力的极限，系统先进性需要高性能芯片支撑。

##### 近十年来，摩尔定律不是以提高频率，而是增加集成规模来延续，体现在FPGA规模的不断增大、CPU多核技术不断发展。

i5-11600K benchmark

![avatar](./i5info.jpg)

6678作为10年前的芯片，6678浮点计算能力，单核1G频率，浮点计算能力16Gflops，这个比例至今也是顶级，但问题是集成度不够，L1/L2/L3缓存小，DDR控制器少（1个），并行访存能力差。

以国产申威3221为例，2G频率，单核32Gflops，集成32核心，8个DDR控制器。

集成度高带来通信及调度开销大大降低，8个青铜(6678)战力之和等于一个黄金(3221)，并不代表8个青铜能够打赢1个黄金。

##### 全网络交换互联，代价是复杂，对AD等实时高带宽数据传输支撑较难（高速AD数据GTX直连，局部全交换，分域，支持横向扩展）。

半导体工艺的发展，大大提升集成度，在同样的计算能力下，可大大降低网络互联复杂度（互联32个6678节点 VS 互联4个3221节点）。

##### 电磁信号处理的特点，实时、高带宽、定制性（架构一致，网络拓扑有区别）。

##### 总线简单对比，万兆以太网类似于普适性更强，但性能较弱的InfiniBand

###### 构建系统便利性，PCIE > RapidIO > InfiniBand 
  
  现在高性能CPU芯片带RapidIO接口的很少，InfiniBand 则几乎没有，都只能做接口转换，而所有CPU都有PCIE。

###### 综合性能：PCIE>=InfiniBand >RapidIO

商用系统，特别是数据中心的广泛应用是性能提升的重要推动力，RapidIO在商用系统中用得较少（基站）。

###### 网络互联扩展性：RapidIO = InfiniBand > PCIE

RapidIO,InfiniBand 节点均可对等通信，PCIE树形结构，但NTB很大程度上规避了这个问题。

###### 国产化状况

RapidIO > PCIE > InfiniBand

RapidIO有全套的接口和交换国产化支持；PCIE交换芯片是短板，有些国产化芯片pcie lane较多，一定程度上缓解；InfiniBand则接口和交换都是短板。
